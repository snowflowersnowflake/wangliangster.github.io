## 极大似然法
> (Maximum Likelihood Method) 中文一般译为极大**似然**法，**"似然"**这个看似文雅的原译生造词，对成功阻碍广大中国学生很好的理解这一方法，起到了决定性的作用(个人观点, 此人应该拉出来鞭尸~)。

幸好，还有聪明且耐心的中国人把这一方法讲清楚了，详见后面的链接。

通俗来说，它是一种估计模型**参数$(\mu,\sigma^2)$**最可能取哪个值合理的方法。类似于**机器学习**通过算法(梯度下降等)找参数($w_0,w_1,...w_n$)，它是指**智人**通过数学方法(解微分求极值等)找参数($\mu,\sigma^2$)的一种方法。

举个**例子:**

假设某一随机事件符合[正态分布](math/statics/files/gauss.md)(经验得来)，我们不知其参数，但有一组观测数据$X={\{x_1,x_2,...x_m\}}$已知，那么怎么根据这些已知数据估计出模型的参数取什么值最合理呢？

首先，这组已知数据肯定是符合假设模型分布的，且它们是相互独立**已经发生**的事实，若把它们看作是多个相互独立随机变量组合成的**一个事件**的一次发生，则它的联合概率就是一种**似然函数**表示， 形如:

$$
L(\mu,\sigma^2|x_1,x_2,...x_m)=P(\mu,\sigma^2|X)=\prod_{i=1}^{m}N(x_i|\mu,\sigma^2)
$$

显然，若某参数使这个“联合概率”最大，则说明该参数与已经发生的观测事实最匹配，获得参数值就是我们的目的。数学上就转化为求函数的极大值问题了。

上面假设中，因正态分布概率密度函数是指数形式，数学技巧上经常把它取对数处理，这样就得到所谓**对偶**问题，其实译为等价问题还更好理解。
$$
L(\mu,\sigma^2|x_1,x_2,...x_m)\varpropto \ln{\prod_{i=1}^{m}N(x_i|\mu,\sigma^2)}
\\=-\sum_{i=1}^{m}{\frac{(x_i-\mu)^2}{2\sigma^2}}-m\ln{\sqrt{2\pi \sigma^2}}
$$
变成多项式和的简单形式函数，令偏导数:

$$
L_{\mu}(\mu,\sigma^2|x_1,x_2,...x_m)=0,\quad L_{\sigma}(\mu,\sigma^2|x_1,x_2,...x_m)=0
$$
很容易，求得方程的**驻点**，一般都为极大值点。

为什么前文说只是**一种**似然函数呢？它还有多种? 答案是**肯定**的。

因为若不以联合概率的方式考虑，就单纯考虑使每个观测数据概率最大，或说使它们的均值出现概率最大，都将得到不同的似然函数:
$$
L(\mu,\sigma^2|x_1,x_2,...x_m)=\sum_{i=1}^{m}N(x_i|\mu,\sigma^2)
$$
和
$$
L(\mu,\sigma^2|x_1,x_2,...x_m)=N(\frac{\sum_{i=1}^{m}x_i}{m}|\mu,\sigma^2)
$$

### 总结

针对不同的似然函数，使用不同的数学技巧，通过使之成为极大值，而获得相应参数的方法，被称为**"极大似然法"**（极其难听的叫法!）

若想结合一些具体例子理解，可参考

- [一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750)
- [详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解](https://blog.csdn.net/u011508640/article/details/72815981)
