## PCA 降维

PCA 降维的应用模型主要基于两种角度的思考：
- 最大化样本的方差（分散度），增加每一个数据辨识度
- 最小化样本间的相关性，减少信息“重叠”表示

PCA 的数学原理主要基于：
- 任一实对称矩阵，都可以对其进行特征分解
- 特征空间的每一个轴对应于一特征值，其大小决定了数据在该维度的伸缩辐度
- 可以根据**应用需要**，选取合适的特征子空间（丢弃大的，或小的，或特定的，或0特征轴，不同应用有不同的考量），它的维度必然减少了，所以称为**降维**

### 举例
对一$m \times n$阶数据集矩阵$A$，常规理解$m$为样本数，$n$为特征数(或说属性)

则 
- $AA'$为$m \times m$阶**样本**间“协方差”（忽略系数，等价）矩阵
- $A'A$为$n \times n$阶**特征**间“协方差”（忽略系数，等价）矩阵

无论从哪个角度思考，根据**特征分解EVD**都可求出其特征向量和相应特征值。
- 若取前$k$个特征向量(列)组成矩阵$P， n \times k$阶， 则$N=AP， m \times k$， 新数据把特征维度降到了$k$
- 若取前$k$个特征向量(行)组成矩阵$P， k \times m$阶，则$N=PA， k \times n$，新数据把样本数降到了$k$（说明有一些样本重复无意义）

### 细节技巧
- 各数据减去均值，相当于正态分布向标准正态分布平移（中心化，简化计算）
- 对于大的维数，可不进行特征分解，借助SVD算法直接得到$P$矩阵
- 至于维数$k$的大小选择，根据公式和应用需求就好
### 参考
- [主成分分析（PCA）原理详解csdn](https://blog.csdn.net/program_developer/article/details/80632779#commentsedit)
- [主成分分析（PCA）原理详解wexin](https://mp.weixin.qq.com/s/Xt1vLQfB20rTmtLjiLsmww)
