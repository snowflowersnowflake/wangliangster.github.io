# 深入理解$L1,L2$正则化(Regularization)

正则化（Regularization）是机器学习中一种常用的技术手段。

其主要作用效果是控制模型复杂度，减少过拟合，有$L1,L2$两种形式。

为什么能达到那样的效果呢？参考文献给出了详细的分析，其各有优缺点，本文就参考中的三篇文章做个简短的阅读反馈点评。

- $J(w,X,y)=(wX-y)^T(wX-y)$ 的二阶Taylor展开（在极少值点$w^{\star}$处）
	- $\hat{J}(w,X,y)=J(w^{\star})+\frac{H(w^{\star})}{2!}(w-w^{\star})^T(w-w^{\star})$， 其中$H(w^{\star})$是$J(w,X,y)$在 $w^{\star}$处的二阶导数(**Hessian矩阵**)，一阶导数为零，因为是极值点。为什么估计的余项省去了？因为$J(w,X,y)$是二次函数，它的三阶以上导数均为零，所以此处估计用$=$号，而不必用$\approx$。也正是因为这个原因，文章理论分析中大胆的使用了$\hat{J}$ 代替真正的正则化函数$\tilde{J}$ 一路狂奔式推导！

	- 参考文章中全部写成$(w-w^{\star})^TH(w-w^{\star})$，让人误以为$H(w-w^{\star})$是函数，矛盾又不敢质疑，很是费解，不应该！

- 因为假设的是极小值点存在， 所以$H(w^{\star})$是正定矩阵（英文文献中有提到）

- $L1$ 两个$case$的理解：
	- 当$|w^{\star}_i|≤\frac{α}{H_{ii}}$时，最原始目标函数$J(w,X,y)$的最优解离约束中心比较近，在约束范畴内，当$w_i=0$时，估计目标函数$\hat{J}$达到最小值，即它是离$w^{\star}$最近的等高线。 
	- 当$|w^{\star}_i|>\frac{α}{H_{ii}}$时, 最原始目标函数$J(w,X,y)$的最优解离约束中心较远，我们在约束边界上滑动，以期找到离最优解最近的等高线，在它未达0之前就很可能已经找到与$w^{\star}$最近的等高线了，再往0移动时，等高线会向外延，离原始最优目标将更远，所以估计目标函数$\hat{J}$的最小值在$|w^{\star}_i|−\frac{α}{H_{i,i}}$处就获得， 此即说明$L1$正则化并不一定产生稀疏矩阵，而是根据$\alpha$条件概率产生。进而我们应该明白了调参的意义是多么的玄学！不断的尝试不同的$\alpha$

- 通过理论分析，应该明白深度学习中不同的$layer$用不同的正则化方式和不同的参数$\alpha$的必要性，按需求不同设定不同，所以说调参真是一件有意义的工作！

## 总结
- $L1$正则化可使解更加靠近某些轴，而其它的轴则为0，所以$L1$正则化能使得到的参数稀疏化,若用$VC$维衡量模型的复杂度，其参数个数得到了压缩，因此控制了模型的复杂度。

- $L2$正则化可使解更加靠近约束超球中心原点，也就是说$L2$正则化能降低参数范数的总和。

- 无论是$L1$，还是$L2$得到的解一定和“完美最优解”有一定偏差，这在一定程度上防止了**过拟和**：即完美匹配训练集，而对于新进的验证集或预测集表现欠佳，不如有一定偏差的模型。

## 参考
- [深入理解L1、L2正则化](https://zhuanlan.zhihu.com/p/29360425)
- [L1正则化及其推导](http://www.cnblogs.com/heguanyou/p/7582578.html)
- [Ian Goodfellow, Yoshua Bengio and Aaron Courville. Deep Learning.](https://www.deeplearningbook.org/contents/regularization.html)
